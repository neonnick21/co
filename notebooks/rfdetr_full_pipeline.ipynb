{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f00ddcea",
   "metadata": {},
   "source": [
    "# RF-DETR Object Detection Project Notebook\n",
    "This notebook covers data exploration, visualization, model training, evaluation, and result analysis for the RF-DETR object detection pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623a39d6",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b66d6221",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image, ImageDraw\n",
    "from src.data.coco_dataset import COCODetectionDataset\n",
    "from src.model.detr_rfdetr import RFDETR\n",
    "from src.model.losses import loss_labels, loss_boxes, loss_giou"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0459ab",
   "metadata": {},
   "source": [
    "## 2. Data Exploration and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8388759d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a split (e.g., train)\n",
    "data_dir = '../data'\n",
    "img_dir = os.path.join(data_dir, 'images', 'train')\n",
    "ann_path = os.path.join(data_dir, 'annotations', 'train.json')\n",
    "dataset = COCODetectionDataset(img_dir, ann_path)\n",
    "print(f'Total training images: {len(dataset)}')\n",
    "# Show a few images with bounding boxes\n",
    "for i in range(3):\n",
    "    image, target = dataset[i]\n",
    "    draw = ImageDraw.Draw(image)\n",
    "    for box in target['boxes']:\n",
    "        draw.rectangle(list(box.numpy()), outline='red', width=2)\n",
    "    plt.imshow(image)\n",
    "    plt.title(f'Image {i}')\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99151899",
   "metadata": {},
   "source": [
    "## 3. Model Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac79f72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 3  # WBC, RBC, Platelets\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = RFDETR(num_classes=num_classes).to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b17c96d",
   "metadata": {},
   "source": [
    "## 4. Training Loop (Single Epoch Example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb863102",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "tfms = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "train_dataset = COCODetectionDataset(img_dir, ann_path, transforms=tfms)\n",
    "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True, collate_fn=collate_fn)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "model.train()\n",
    "for images, targets in train_loader:\n",
    "    images = torch.stack(images).to(device)\n",
    "    # TODO: Implement Hungarian matcher and loss calculation\n",
    "    outputs = model(images)\n",
    "    # Placeholder: zero loss\n",
    "    loss = torch.tensor(0.0, requires_grad=True).to(device)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    print('Batch done')\n",
    "    break  # Remove for full epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4076222b",
   "metadata": {},
   "source": [
    "## 5. Model Evaluation and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e436ac88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model weights if available\n",
    "model_path = '../rfdetr_best.pth'\n",
    "if os.path.exists(model_path):\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "model.eval()\n",
    "# Evaluate on a few test images\n",
    "test_img_dir = os.path.join(data_dir, 'images', 'test')\n",
    "test_ann_path = os.path.join(data_dir, 'annotations', 'test.json')\n",
    "test_dataset = COCODetectionDataset(test_img_dir, test_ann_path, transforms=tfms)\n",
    "for i in range(3):\n",
    "    image, target = test_dataset[i]\n",
    "    with torch.no_grad():\n",
    "        output = model(image.unsqueeze(0).to(device))\n",
    "    pred_boxes = output['pred_boxes'].cpu().numpy()[0]\n",
    "    pred_logits = output['pred_logits'].cpu().numpy()[0]\n",
    "    scores = pred_logits.max(axis=1)\n",
    "    labels = pred_logits.argmax(axis=1)\n",
    "    keep = scores > 0.5\n",
    "    pred_boxes = pred_boxes[keep]\n",
    "    labels = labels[keep]\n",
    "    draw = ImageDraw.Draw(image)\n",
    "    for box in pred_boxes:\n",
    "        draw.rectangle(list(box), outline='blue', width=2)\n",
    "    plt.imshow(image)\n",
    "    plt.title(f'Test Image {i}')\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4aff27",
   "metadata": {},
   "source": [
    "## 6. mAP Calculation (pycocotools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77770c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycocotools.coco import COCO\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "# TODO: Format predictions as COCO results and save to JSON\n",
    "# pred_json = 'predictions.json'\n",
    "# ann_file = test_ann_path\n",
    "# coco_gt = COCO(ann_file)\n",
    "# coco_dt = coco_gt.loadRes(pred_json)\n",
    "# coco_eval = COCOeval(coco_gt, coco_dt, 'bbox')\n",
    "# coco_eval.evaluate()\n",
    "# coco_eval.accumulate()\n",
    "# coco_eval.summarize()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
